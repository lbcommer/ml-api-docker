{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Machine learning model to get personality insights from text. It is based on this [Kaggle dataset](https://www.kaggle.com/datasnaek/mbti-type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/mbti-myers-briggs-personality-type-dataset.zip', \n",
    "                   compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti = {'I':'Introversion', 'E':'Extroversion', 'N':'Intuition', \n",
    "        'S':'Sensing', 'T':'Thinking', 'F': 'Feeling', \n",
    "        'J':'Judging', 'P': 'Perceiving'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(text):\n",
    "    text = BeautifulSoup(text, \"lxml\").text\n",
    "    text = re.sub(r'\\|\\|\\|', r' ', text) \n",
    "    text = re.sub(r'http\\S+', r'<URL>', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['clean_posts'] = train['posts'].apply(cleanText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "scoring = {'acc': 'accuracy',\n",
    "           'neg_log_loss': 'neg_log_loss',\n",
    "           'f1_micro': 'f1_micro'}\n",
    "\n",
    "tfidf2 = CountVectorizer(ngram_range=(1, 1), \n",
    "                         stop_words='english',\n",
    "                         lowercase = True, \n",
    "                         max_features = 5000)\n",
    "\n",
    "\n",
    "\n",
    "model_lr = Pipeline([('tfidf1', tfidf2), \n",
    "                     ('lr', LogisticRegression(class_weight=\"balanced\", C=0.005))])\n",
    "\n",
    "results_lr = cross_validate(model_lr, train['clean_posts'], train['type'], cv=kfolds, \n",
    "                          scoring=scoring, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy: 0.6659 (+/- 0.0090)\n",
      "CV F1: 0.6659 (+/- 0.0090)\n",
      "CV Logloss: 1.2501 (+/- 0.0238)\n"
     ]
    }
   ],
   "source": [
    "print(\"CV Accuracy: {:0.4f} (+/- {:0.4f})\".format(np.mean(results_lr['test_acc']),\n",
    "                                                          np.std(results_lr['test_acc'])))\n",
    "\n",
    "print(\"CV F1: {:0.4f} (+/- {:0.4f})\".format(np.mean(results_lr['test_f1_micro']),\n",
    "                                                          np.std(results_lr['test_f1_micro'])))\n",
    "\n",
    "print(\"CV Logloss: {:0.4f} (+/- {:0.4f})\".format(np.mean(-1*results_lr['test_neg_log_loss']),\n",
    "                                                          np.std(-1*results_lr['test_neg_log_loss'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's integrate preprocessing into the pipeline to try this trick (a easier way could have been in this case to use the preprocessing param of the CountVectorizer class):  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipelinize(function, active=True):\n",
    "    def list_comprehend_a_function(list_or_series, active=True):\n",
    "        if active:\n",
    "            return [function(i) for i in list_or_series]\n",
    "        else: # if it's not active, just pass it right back\n",
    "            return list_or_series\n",
    "    return FunctionTransformer(list_comprehend_a_function, validate=False, kw_args={'active':active})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([('prepro', pipelinize(cleanText)), \n",
    "                  ('tfidf1', tfidf2), \n",
    "                  ('model', LogisticRegression(class_weight=\"balanced\", C=0.005))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('prepro', FunctionTransformer(accept_sparse=False,\n",
       "          func=<function pipelinize.<locals>.list_comprehend_a_function at 0x110db5730>,\n",
       "          inv_kw_args=None, inverse_func=None, kw_args={'active': True},\n",
       "          pass_y='deprecated', validate=False)), ('tfidf1', CountVectorizer(ana...ty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train['posts'], train['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ISFJ'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([\"hello my friend\"])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the model to be used later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../model/model.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it will work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../model/model.pkl', 'rb') as file:\n",
    "    model_loaded = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ISFJ'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded.predict([\"hello my friend\"])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "- https://towardsdatascience.com/a-flask-api-for-serving-scikit-learn-models-c8bcdaa41daa\n",
    "- https://www.analyticsvidhya.com/blog/2017/09/machine-learning-models-as-apis-using-flask/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
